{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "from scipy.stats import pearsonr\n",
    "import sklearn\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as n\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, IterativeImputer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.utils import resample\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import joblib\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_path):\n",
    "    return pd.read_excel(file_path)\n",
    "\n",
    "def display_data(data):\n",
    "    return data.head()\n",
    "\n",
    "def drop_columns(data, columns, axis):\n",
    "    return data.drop(columns=columns, axis=axis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check distribution of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_distribution(data):\n",
    "    fig, axes = plt.subplots(3, 5, figsize=(20,10), sharey=True)\n",
    "    axes = axes.ravel()\n",
    "    for i, col in enumerate(data.columns[0:13]):\n",
    "        if col == 'RelapseFreeSurvival (outcome)' or col == 'Age':\n",
    "            sns.histplot(data=data, x=col, ax=axes[i])\n",
    "        else:\n",
    "            sns.countplot(data=data, x=col, ax=axes[i])\n",
    "        axes[i].set_title(f'Count Plot for {col}')\n",
    "        axes[i].set_xlabel(col)\n",
    "        axes[i].set_ylabel('Count')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_nan_values(data):\n",
    "    \"\"\"Replace 999 values with NaN and return the modified DataFrame.\"\"\"\n",
    "    data_copy = data.copy()\n",
    "    data_copy.replace(999, np.nan, inplace=True)\n",
    "    return data_copy\n",
    "\n",
    "def get_description_data(data):\n",
    "    return data.iloc[:, 0:13].info()\n",
    "\n",
    "def identify_categorical_continuous_features(data):\n",
    "    \"\"\"Identify categorical and continuous features based on data type and unique values.\"\"\"\n",
    "    # First, separate numeric and non-numeric columns\n",
    "    numeric_columns = data.select_dtypes(include=['int64', 'float64']).columns\n",
    "    non_numeric_columns = data.select_dtypes(exclude=['int64', 'float64']).columns\n",
    "    \n",
    "    # Among numeric columns, identify categorical (few unique values) and continuous\n",
    "    categorical_features = [\n",
    "        col for col in numeric_columns\n",
    "        if data[col].nunique() < 20\n",
    "    ] + list(non_numeric_columns)  # Add all non-numeric columns to categorical\n",
    "    \n",
    "    continuous_features = [\n",
    "        col for col in numeric_columns\n",
    "        if col not in categorical_features\n",
    "    ]\n",
    "    \n",
    "    return categorical_features, continuous_features\n",
    "\n",
    "def check_missing_data(data):\n",
    "    null_counts = data.isnull().sum()\n",
    "    null_counts = null_counts[null_counts > 0]\n",
    "    null_percentage = null_counts / len(data)\n",
    "\n",
    "    return null_counts, null_percentage\n",
    "\n",
    "def init_imputers():\n",
    "    mice_imputer = IterativeImputer(max_iter=50, random_state=0)\n",
    "    mean_imputer = SimpleImputer(strategy='mean')\n",
    "    return mice_imputer, mean_imputer\n",
    "\n",
    "def impute_features_and_target(data, target_column, categorical_features, continuous_features):\n",
    "    \"\"\"Handle missing values for both features and target.\"\"\"\n",
    "    # Separate features and target\n",
    "    X = data.drop(target_column, axis=1)\n",
    "    y = data[target_column]\n",
    "    \n",
    "    # Impute features\n",
    "    X_imputed = impute_features(X, categorical_features, continuous_features)\n",
    "    \n",
    "    # Impute target\n",
    "    y_imputed = impute_target(y)\n",
    "    \n",
    "    # Combine imputed features and target\n",
    "    final_data = X_imputed.copy()\n",
    "    final_data[target_column] = y_imputed\n",
    "    \n",
    "    return final_data\n",
    "\n",
    "def impute_features(X, categorical_features, continuous_features):\n",
    "    \"\"\"Impute missing values in features using appropriate strategies.\"\"\"\n",
    "    # Set up column transformer\n",
    "    column_transformer = set_column_transformer(categorical_features, continuous_features)\n",
    "    \n",
    "    # Apply imputation\n",
    "    X_imputed = apply_imputation(X, column_transformer)\n",
    "    \n",
    "    # Convert back to DataFrame\n",
    "    return convert_to_dataframe(X_imputed, categorical_features, continuous_features)\n",
    "\n",
    "def impute_target(y):\n",
    "    \"\"\"Impute missing values in target using mode strategy.\"\"\"\n",
    "    target_imputer = SimpleImputer(strategy='most_frequent')\n",
    "    y_imputed = target_imputer.fit_transform(y.values.reshape(-1, 1))\n",
    "    return y_imputed.ravel().astype(int)\n",
    "\n",
    "def set_column_transformer(categorical_features, continuous_features):\n",
    "    \"\"\"Set up column transformer with appropriate imputation strategies.\"\"\"\n",
    "    # Initialize imputers\n",
    "    categorical_imputer = SimpleImputer(strategy='most_frequent')  # Use mode for categorical\n",
    "    continuous_imputer = SimpleImputer(strategy='mean')  # Use mean for continuous\n",
    "    \n",
    "    # Create transformers list\n",
    "    transformers = []\n",
    "    \n",
    "    # Add categorical imputer if there are categorical features\n",
    "    if categorical_features:\n",
    "        transformers.append(('cat', categorical_imputer, categorical_features))\n",
    "    \n",
    "    # Add continuous imputer if there are continuous features\n",
    "    if continuous_features:\n",
    "        transformers.append(('cont', continuous_imputer, continuous_features))\n",
    "    \n",
    "    # Create and return column transformer\n",
    "    return ColumnTransformer(\n",
    "        transformers=transformers,\n",
    "        remainder='passthrough'\n",
    "    )\n",
    "\n",
    "def apply_imputation(data, column_transformer):\n",
    "    \"\"\"Apply the column transformer to impute missing values.\"\"\"\n",
    "    return column_transformer.fit_transform(data)\n",
    "\n",
    "def convert_to_dataframe(data, categorical_features, continuous_features):\n",
    "    \"\"\"Convert imputed array back to DataFrame with proper column names.\"\"\"\n",
    "    # Combine all feature names in the correct order\n",
    "    all_features = categorical_features + continuous_features\n",
    "    \n",
    "    # Create DataFrame with original column names\n",
    "    return pd.DataFrame(data, columns=all_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_outliers(data):\n",
    "    # Example Z-score threshold for detecting extreme outliers\n",
    "    z_threshold = 3\n",
    "\n",
    "    # Calculate the number and percentage of extreme outliers for each column\n",
    "    extreme_outliers_info = {}\n",
    "\n",
    "    for col in data.select_dtypes(include=['int64', 'float64']).columns[1:]:\n",
    "        z_scores = np.abs((data[col] - data[col].mean()) / data[col].std())\n",
    "        outliers_count = (z_scores > z_threshold).sum()\n",
    "        total_count = len(data[col].dropna())\n",
    "        outliers_percentage = (outliers_count / total_count) * 100\n",
    "        extreme_outliers_info[col] = {\n",
    "            'count': outliers_count,\n",
    "            'max_z_score': z_scores.max(),  # Track the maximum Z-score for the most extreme outlier\n",
    "            'percentage': outliers_percentage,\n",
    "            'outlier_indices': data[col].index[z_scores > z_threshold]  # Indices of outliers\n",
    "        }\n",
    "\n",
    "    # Sort columns by the maximum Z-score to ensure we get the most extreme outliers\n",
    "    sorted_columns = sorted(\n",
    "        extreme_outliers_info, \n",
    "        key=lambda x: (extreme_outliers_info[x]['count'], extreme_outliers_info[x]['max_z_score']), \n",
    "        reverse=True\n",
    "    )\n",
    "\n",
    "    # Choose the top N columns to visualize (e.g., top 24)\n",
    "    top_columns = sorted_columns[:24]\n",
    "\n",
    "    # Adjust the number of rows/columns for up to 24 plots (e.g., 6 rows, 4 columns)\n",
    "    fig, axes = plt.subplots(nrows=6, ncols=4, figsize=(20, 30))\n",
    "    fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "\n",
    "    axes = axes.ravel()\n",
    "\n",
    "    # Plot boxplots for the top columns with the most extreme outliers\n",
    "    for i, col in enumerate(top_columns):\n",
    "        outliers_count = extreme_outliers_info[col]['count']\n",
    "        outliers_percentage = extreme_outliers_info[col]['percentage']\n",
    "        \n",
    "        # Create the boxplot\n",
    "        sns.boxplot(y=col, x='pCR (outcome)', data=data, ax=axes[i], showfliers=False)  # Hide default fliers\n",
    "        \n",
    "        # Plot outliers in red\n",
    "        outlier_indices = extreme_outliers_info[col]['outlier_indices']\n",
    "        outliers = data.loc[outlier_indices]\n",
    "        sns.scatterplot(\n",
    "            y=outliers[col], \n",
    "            x=outliers['pCR (outcome)'], \n",
    "            ax=axes[i], \n",
    "            color='red', \n",
    "            edgecolor='w', \n",
    "            s=50, \n",
    "            alpha=0.8, \n",
    "            label='Outliers'\n",
    "        )\n",
    "        \n",
    "        # Add title with outlier information\n",
    "        axes[i].set_title(\n",
    "            f\"Column: {col}\\nOutliers: {outliers_count} ({outliers_percentage:.2f}%)\"\n",
    "        )\n",
    "\n",
    "    # Hide any unused subplots if you have fewer top columns than subplots\n",
    "    for j in range(len(top_columns), len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def count_outliers_using_iqr_range(data, iqr_multiple=1.5):\n",
    "    outliers_dict = {}\n",
    "    \n",
    "    for column in data.select_dtypes(include=['int64', 'float64']).columns:\n",
    "        try:\n",
    "            Q1 = data[column].quantile(0.25)\n",
    "            Q3 = data[column].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - iqr_multiple * IQR\n",
    "            upper_bound = Q3 + iqr_multiple * IQR\n",
    "            iqr_outliers = data[column][(data[column] < lower_bound) | (data[column] > upper_bound)]\n",
    "            outliers_dict[column] = len(iqr_outliers)\n",
    "        except:\n",
    "            outliers_dict[column] = 0\n",
    "    \n",
    "    return outliers_dict\n",
    "\n",
    "def count_outliers_using_z_score(data, z_threshold=3):\n",
    "    outliers_dict = {}\n",
    "    \n",
    "    for column in data.select_dtypes(include=['int64', 'float64']).columns:\n",
    "        try:\n",
    "            column_data = data[column].dropna()\n",
    "            z_scores = np.abs((column_data - column_data.mean()) / column_data.std())\n",
    "            z_score_outliers = column_data[z_scores > z_threshold]\n",
    "            outliers_dict[column] = len(z_score_outliers)\n",
    "        except:\n",
    "            outliers_dict[column] = 0\n",
    "    \n",
    "    return outliers_dict\n",
    "\n",
    "def handle_z_score_outliers(data, method='clip', z_threshold=3):\n",
    "    df_processed = data.copy()\n",
    "    \n",
    "    for column in data.select_dtypes(include=['int64', 'float64']).columns:\n",
    "        try:\n",
    "            column_data = data[column].dropna()\n",
    "            z_scores = np.abs((column_data - column_data.mean()) / column_data.std())\n",
    "            extreme_outlier_mask = z_scores > z_threshold\n",
    "            \n",
    "            if method == 'clip':\n",
    "                # Clip extreme values to z-score threshold boundaries\n",
    "                mean = column_data.mean()\n",
    "                std = column_data.std()\n",
    "                df_processed[column] = df_processed[column].apply(\n",
    "                    lambda x: min(max(x, mean - z_threshold*std), mean + z_threshold*std)\n",
    "                )\n",
    "            \n",
    "            elif method == 'remove':\n",
    "                # Remove rows with extreme outliers\n",
    "                df_processed = df_processed[~extreme_outlier_mask]\n",
    "            \n",
    "            elif method == 'winsorize':\n",
    "                # Winsorize extreme values\n",
    "                df_processed[column] = stats.mstats.winsorize(\n",
    "                    df_processed[column], \n",
    "                    limits=[0.05, 0.05]\n",
    "                )\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing column {column}: {e}\")\n",
    "    \n",
    "    return df_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data(X, y, minority_class=1, majority_class=0, strategy='smote'):\n",
    "    \"\"\"\n",
    "    Augment the dataset to handle class imbalance.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : pandas.DataFrame\n",
    "        Feature matrix\n",
    "    y : pandas.Series\n",
    "        Target variable\n",
    "    minority_class : int, default=1\n",
    "        Label of the minority class\n",
    "    majority_class : int, default=0\n",
    "        Label of the majority class\n",
    "    strategy : str, default='smote'\n",
    "        Strategy to use for augmentation ('smote' or 'upsample')\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple : (X_balanced, y_balanced)\n",
    "        Balanced dataset\n",
    "    \"\"\"\n",
    "    if strategy == 'smote':\n",
    "        from imblearn.over_sampling import SMOTE\n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_balanced, y_balanced = smote.fit_resample(X, y)\n",
    "        \n",
    "    elif strategy == 'upsample':\n",
    "        # Combine features and target for resampling\n",
    "        data = pd.concat([X, pd.Series(y, name='target')], axis=1)\n",
    "        \n",
    "        # Separate majority and minority classes\n",
    "        majority = data[data.target == majority_class]\n",
    "        minority = data[data.target == minority_class]\n",
    "        \n",
    "        # Upsample minority class\n",
    "        minority_upsampled = resample(\n",
    "            minority,\n",
    "            replace=True,\n",
    "            n_samples=len(majority),\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        # Combine majority and upsampled minority\n",
    "        data_balanced = pd.concat([majority, minority_upsampled])\n",
    "        \n",
    "        # Separate features and target\n",
    "        X_balanced = data_balanced.drop('target', axis=1)\n",
    "        y_balanced = data_balanced.target\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown strategy: {strategy}\")\n",
    "    \n",
    "    return X_balanced, y_balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling and normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise_data(data, continuous_features):\n",
    "    \"\"\"Normalize only the continuous features while preserving categorical ones.\"\"\"\n",
    "    # Create a copy of the data\n",
    "    df_normalized = data.copy()\n",
    "    \n",
    "    # Only normalize continuous features\n",
    "    if continuous_features:\n",
    "        scaler = StandardScaler()\n",
    "        df_normalized[continuous_features] = scaler.fit_transform(data[continuous_features])\n",
    "    \n",
    "    return df_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection and dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_heatmap(data):\n",
    "    matrix = data.iloc[:, 0:12].corr(numeric_only=True)\n",
    "    # Generate a mask for the upper triangle\n",
    "    mask = np.zeros_like(matrix)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "    # Set up the matplotlib figure\n",
    "    fig, ax = plt.subplots(figsize=(15, 8))\n",
    "    plt.title('Clinical Features Correlation')\n",
    "    # Draw the heatmap with the mask and correct aspect ratio\n",
    "    heatmap = sns.heatmap(matrix, vmax=1.2, square=False, cmap='crest', mask=mask, ax=ax, annot=True, fmt='.2g', linewidths=1)\n",
    "    return heatmap\n",
    "\n",
    "def apply_pca(data, target_column, is_training=True, pca_model=None):\n",
    "    \"\"\"\n",
    "    Apply PCA transformation while preserving clinical features and target.\n",
    "    \"\"\"\n",
    "    # Separate target\n",
    "    target = data[target_column]\n",
    "    \n",
    "    # Separate clinical and MRI features\n",
    "    clinical_features = data.iloc[:, 0:12]\n",
    "    mri_features = data.iloc[:, 12:]\n",
    "    \n",
    "    if is_training:\n",
    "        # Initialize and fit PCA on the training data\n",
    "        pca = PCA(n_components=0.95)\n",
    "        data_pca = pca.fit_transform(mri_features)\n",
    "        \n",
    "        # Create DataFrame with meaningful column names for PCA components\n",
    "        pca_columns = [f'PCA_Component_{i+1}' for i in range(data_pca.shape[1])]\n",
    "        data_pca = pd.DataFrame(data=data_pca, columns=pca_columns)\n",
    "        \n",
    "        # Save column names for test data\n",
    "        feature_names = list(clinical_features.columns) + list(data_pca.columns)\n",
    "        \n",
    "        # Combine all parts\n",
    "        data_transformed = pd.concat([clinical_features, data_pca, target], axis=1)\n",
    "        return data_transformed, pca, feature_names\n",
    "    else:\n",
    "        # Transform test data using saved PCA model\n",
    "        data_pca = pca_model.transform(mri_features)\n",
    "        \n",
    "        # Create DataFrame with same column names as training\n",
    "        n_components = pca_model.n_components_\n",
    "        pca_columns = [f'PCA_Component_{i+1}' for i in range(n_components)]\n",
    "        data_pca = pd.DataFrame(data=data_pca, columns=pca_columns)\n",
    "        \n",
    "        # Combine all parts\n",
    "        data_transformed = pd.concat([clinical_features, data_pca, target], axis=1)\n",
    "        return data_transformed\n",
    "\n",
    "def display_pca_variance(data):\n",
    "    data, pca = apply_pca(data)\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(np.cumsum(pca.explained_variance_ratio_), marker='o')\n",
    "    plt.title('Cumulative Explained Variance')\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.ylabel('Cumulative Explained Variance')\n",
    "    plt.grid()\n",
    "    return plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(data, target_column, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Split dataset into training and testing sets.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pandas.DataFrame\n",
    "        The complete dataset including features and target\n",
    "    target_column : str\n",
    "        Name of the target column\n",
    "    test_size : float, default=0.2\n",
    "        Proportion of the dataset to include in the test split\n",
    "    random_state : int, default=42\n",
    "        Random state for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple : (X_train, X_test, y_train, y_test)\n",
    "        Split datasets for training and testing\n",
    "    \"\"\"\n",
    "    # Split features and target\n",
    "    X = data.drop(target_column, axis=1)\n",
    "    y = data[target_column]\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, \n",
    "        test_size=test_size, \n",
    "        random_state=random_state,\n",
    "        stratify=y  # Ensure balanced split for classification\n",
    "    )\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dataset and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(file_path, target_column, is_training=True):\n",
    "    \"\"\"\n",
    "    Load and prepare the initial dataset.\n",
    "    \"\"\"\n",
    "    # Read data\n",
    "    data = read_data(file_path)\n",
    "    \n",
    "    # Replace 999 with NaN\n",
    "    data = replace_nan_values(data)\n",
    "    \n",
    "    # Drop ID column if it exists\n",
    "    if 'ID' in data.columns:\n",
    "        data = drop_columns(data.copy(), ['ID'], 1)\n",
    "    \n",
    "    if is_training:\n",
    "        # Process target variable\n",
    "        data = prepare_target(data, target_column)\n",
    "        # Save column names for later use\n",
    "        os.makedirs('./models/data_preprocessing', exist_ok=True)\n",
    "        feature_columns = [col for col in data.columns if col != target_column]\n",
    "        joblib.dump(feature_columns, './models/data_preprocessing/original_columns.joblib')\n",
    "    else:\n",
    "        # Load expected columns from training\n",
    "        try:\n",
    "            expected_columns = joblib.load('./models/data_preprocessing/original_columns.joblib')\n",
    "            # Add missing columns with zeros\n",
    "            for col in expected_columns:\n",
    "                if col not in data.columns:\n",
    "                    data[col] = 0\n",
    "            # Add target column with dummy values for test data\n",
    "            data[target_column] = 0\n",
    "            # Reorder columns to match training data\n",
    "            data = data[expected_columns + [target_column]]\n",
    "        except FileNotFoundError:\n",
    "            raise ValueError(\"Column information not found. Please run training pipeline first.\")\n",
    "    \n",
    "    # Identify feature types\n",
    "    feature_data = drop_columns(data.copy(), target_column, 1)\n",
    "    categorical_features, continuous_features = identify_categorical_continuous_features(feature_data)\n",
    "    \n",
    "    print(f\"\\nDataset Summary:\")\n",
    "    print(f\"Shape: {data.shape}\")\n",
    "    \n",
    "    return data, categorical_features, continuous_features\n",
    "\n",
    "def prepare_target(data, target_column):\n",
    "    \"\"\"\n",
    "    Prepare target variable by handling missing values and converting to proper type.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pandas.DataFrame\n",
    "        Input dataset\n",
    "    target_column : str\n",
    "        Name of the target column\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        Dataset with processed target variable\n",
    "    \"\"\"\n",
    "    # Convert target to integer type, temporarily using -1 for NaN\n",
    "    data[target_column] = data[target_column].fillna(-1).astype(int)\n",
    "    \n",
    "    # Replace -1 back to NaN\n",
    "    data.loc[data[target_column] == -1, target_column] = np.nan\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying the preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocessingPipeline:\n",
    "    def __init__(self, file_path, is_training, target_column, augmentation_strategy=None):\n",
    "        \"\"\"Initialize the pipeline with the data file path.\"\"\"\n",
    "        self.file_path = file_path\n",
    "        self.is_training = is_training\n",
    "        self.augmentation_strategy = augmentation_strategy\n",
    "        self.data = None\n",
    "        self.X_train = None\n",
    "        self.X_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "        self.categorical_features = None\n",
    "        self.continuous_features = None\n",
    "        self.target_vars = None\n",
    "        self.target_column = target_column\n",
    "        self.pca_model = None\n",
    "        self.feature_names = None\n",
    "        \n",
    "    def load_and_prepare_data(self):\n",
    "        \"\"\"Load and prepare the initial dataset.\"\"\"\n",
    "        \n",
    "        # Prepare dataset\n",
    "        self.data, self.categorical_features, self.continuous_features = prepare_dataset(\n",
    "            file_path=self.file_path,\n",
    "            target_column=self.target_column,\n",
    "            is_training=self.is_training\n",
    "        )\n",
    "        \n",
    "        if self.is_training:\n",
    "            # Save feature information\n",
    "            self.feature_names = [col for col in self.data.columns if col != self.target_column]\n",
    "            os.makedirs('./models/data_preprocessing', exist_ok=True)\n",
    "            joblib.dump(self.feature_names, './models/data_preprocessing/feature_names.joblib')\n",
    "        else:\n",
    "            # Load feature names from training\n",
    "            self.feature_names = joblib.load('./models/data_preprocessing/feature_names.joblib')\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def handle_missing_values(self):\n",
    "        \"\"\"Handle missing values using appropriate imputation strategies.\"\"\"\n",
    "        self.data = impute_features_and_target(\n",
    "            self.data,\n",
    "            self.target_column,\n",
    "            self.categorical_features,\n",
    "            self.continuous_features\n",
    "        )\n",
    "        return self\n",
    "    \n",
    "    def handle_outliers(self, method='clip', z_threshold=3):\n",
    "        \"\"\"Handle outliers using the specified method.\"\"\"\n",
    "        self.data = handle_z_score_outliers(\n",
    "            self.data, \n",
    "            method=method, \n",
    "            z_threshold=z_threshold\n",
    "        )\n",
    "        return self\n",
    "    \n",
    "    def augment_training_data(self):\n",
    "        \"\"\"Apply data augmentation to training data if specified.\"\"\"\n",
    "        if not self.is_training or not self.augmentation_strategy:\n",
    "            return self\n",
    "            \n",
    "        print(f\"\\nApplying {self.augmentation_strategy} augmentation...\")\n",
    "        print(f\"Before augmentation - Shape: {self.data.shape}\")\n",
    "        print(f\"Class distribution:\\n{self.data[self.target_column].value_counts()}\")\n",
    "        \n",
    "        # Separate features and target\n",
    "        X = self.data.drop(self.target_column, axis=1)\n",
    "        y = self.data[self.target_column]\n",
    "        \n",
    "        # Apply augmentation\n",
    "        X_balanced, y_balanced = augment_data(\n",
    "            X, y, \n",
    "            strategy=self.augmentation_strategy\n",
    "        )\n",
    "        \n",
    "        # Combine back into DataFrame\n",
    "        self.data = pd.concat([\n",
    "            pd.DataFrame(X_balanced, columns=X.columns),\n",
    "            pd.Series(y_balanced, name=self.target_column)\n",
    "        ], axis=1)\n",
    "        \n",
    "        print(f\"After augmentation - Shape: {self.data.shape}\")\n",
    "        print(f\"Class distribution:\\n{self.data[self.target_column].value_counts()}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def normalize_features(self):\n",
    "        \"\"\"Apply normalization to the features.\"\"\"\n",
    "        self.data = normalise_data(self.data, self.continuous_features)\n",
    "        return self\n",
    "    \n",
    "    def reduce_dimensions(self):\n",
    "        \"\"\"Apply PCA to reduce dimensionality of MRI features.\"\"\"\n",
    "        os.makedirs('./models/data_preprocessing', exist_ok=True)\n",
    "        \n",
    "        if self.is_training:\n",
    "            # For training data: fit and save PCA model\n",
    "            self.data, self.pca_model, self.feature_names = apply_pca(data=self.data, target_column=self.target_column, is_training=True)\n",
    "            \n",
    "            # Save PCA model and feature names\n",
    "            joblib.dump(self.pca_model, './models/data_preprocessing/pca_model.joblib')\n",
    "            joblib.dump(self.feature_names, './models/data_preprocessing/feature_names.joblib')\n",
    "        else:\n",
    "            # For test data: load saved PCA model and transform\n",
    "            try:\n",
    "                self.pca_model = joblib.load('./models/data_preprocessing/pca_model.joblib')\n",
    "                self.feature_names = joblib.load('./models/data_preprocessing/feature_names.joblib')\n",
    "                self.data = apply_pca(data=self.data, target_column=self.target_column, is_training=False, pca_model=self.pca_model)\n",
    "                \n",
    "                # Ensure columns match training data\n",
    "                missing_cols = set(self.feature_names) - set(self.data.columns)\n",
    "                for col in missing_cols:\n",
    "                    self.data[col] = 0\n",
    "                \n",
    "                # Reorder columns to match training\n",
    "                self.data = self.data[self.feature_names + ['pCR (outcome)']]\n",
    "                \n",
    "            except FileNotFoundError:\n",
    "                raise ValueError(\"PCA model not found. Please run training pipeline first.\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def split_data(self, test_size=0.2, random_state=42):\n",
    "        \"\"\"Split the data into training and testing sets.\"\"\"\n",
    "        if self.is_training:\n",
    "            self.X_train, self.X_test, self.y_train, self.y_test = split_dataset(\n",
    "                self.data,\n",
    "                self.target_column,\n",
    "                test_size=test_size,\n",
    "                random_state=random_state\n",
    "            )\n",
    "        else:\n",
    "            # For test data, don't split - just separate features and target\n",
    "            self.X_test = self.data.drop(self.target_column, axis=1)\n",
    "            self.y_test = self.data[self.target_column]\n",
    "            # Set other attributes to None for test data\n",
    "            self.X_train = None\n",
    "            self.y_train = None\n",
    "        return self\n",
    "    \n",
    "    def get_processed_data(self):\n",
    "        \"\"\"Return the processed and split data.\"\"\"\n",
    "        if self.is_training:\n",
    "            return self.X_train, self.X_test, self.y_train, self.y_test\n",
    "        else:\n",
    "            # For test data, return None for training data\n",
    "            return None, self.X_test, None, self.y_test\n",
    "    \n",
    "    def run_pipeline(self):\n",
    "        \"\"\"Run the complete preprocessing pipeline.\"\"\"\n",
    "        (self\n",
    "        .load_and_prepare_data()\n",
    "        .handle_missing_values()\n",
    "        .handle_outliers()\n",
    "        .augment_training_data()\n",
    "        .normalize_features()\n",
    "        .reduce_dimensions()\n",
    "        .split_data()\n",
    "        )\n",
    "        return self.get_processed_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Summary:\n",
      "Shape: (400, 120)\n",
      "\n",
      "Applying smote augmentation...\n",
      "Before augmentation - Shape: (400, 120)\n",
      "Class distribution:\n",
      "pCR (outcome)\n",
      "0    316\n",
      "1     84\n",
      "Name: count, dtype: int64\n",
      "After augmentation - Shape: (632, 120)\n",
      "Class distribution:\n",
      "pCR (outcome)\n",
      "1    316\n",
      "0    316\n",
      "Name: count, dtype: int64\n",
      "New dataset shape: (632, 27)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ER</th>\n",
       "      <th>PgR</th>\n",
       "      <th>HER2</th>\n",
       "      <th>TrippleNegative</th>\n",
       "      <th>ChemoGrade</th>\n",
       "      <th>Proliferation</th>\n",
       "      <th>HistologyType</th>\n",
       "      <th>LNStatus</th>\n",
       "      <th>TumourStage</th>\n",
       "      <th>Gene</th>\n",
       "      <th>...</th>\n",
       "      <th>PCA_Component_6</th>\n",
       "      <th>PCA_Component_7</th>\n",
       "      <th>PCA_Component_8</th>\n",
       "      <th>PCA_Component_9</th>\n",
       "      <th>PCA_Component_10</th>\n",
       "      <th>PCA_Component_11</th>\n",
       "      <th>PCA_Component_12</th>\n",
       "      <th>PCA_Component_13</th>\n",
       "      <th>PCA_Component_14</th>\n",
       "      <th>pCR (outcome)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>612</th>\n",
       "      <td>0.132801</td>\n",
       "      <td>0.132801</td>\n",
       "      <td>0.132801</td>\n",
       "      <td>0.867199</td>\n",
       "      <td>2.734398</td>\n",
       "      <td>1.867199</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.265602</td>\n",
       "      <td>0.867199</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.255615</td>\n",
       "      <td>2.306287</td>\n",
       "      <td>0.302505</td>\n",
       "      <td>-0.734249</td>\n",
       "      <td>-0.410544</td>\n",
       "      <td>-0.111068</td>\n",
       "      <td>0.312285</td>\n",
       "      <td>-0.807706</td>\n",
       "      <td>-1.778402</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.758432</td>\n",
       "      <td>0.369647</td>\n",
       "      <td>-0.500542</td>\n",
       "      <td>-0.727722</td>\n",
       "      <td>-0.911664</td>\n",
       "      <td>0.901332</td>\n",
       "      <td>-0.808491</td>\n",
       "      <td>-0.902036</td>\n",
       "      <td>-1.457106</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>3.077975</td>\n",
       "      <td>1.328483</td>\n",
       "      <td>-4.604421</td>\n",
       "      <td>0.566355</td>\n",
       "      <td>-1.433317</td>\n",
       "      <td>-2.189594</td>\n",
       "      <td>1.413369</td>\n",
       "      <td>0.157920</td>\n",
       "      <td>-3.178941</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.379061</td>\n",
       "      <td>0.275397</td>\n",
       "      <td>1.327597</td>\n",
       "      <td>0.115218</td>\n",
       "      <td>0.247911</td>\n",
       "      <td>0.802660</td>\n",
       "      <td>0.118586</td>\n",
       "      <td>-0.693596</td>\n",
       "      <td>0.367737</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613</th>\n",
       "      <td>0.636404</td>\n",
       "      <td>0.636404</td>\n",
       "      <td>0.636404</td>\n",
       "      <td>0.363596</td>\n",
       "      <td>1.727193</td>\n",
       "      <td>1.363596</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.363596</td>\n",
       "      <td>2.272807</td>\n",
       "      <td>0.363596</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.416776</td>\n",
       "      <td>-0.303488</td>\n",
       "      <td>-0.289124</td>\n",
       "      <td>-0.325761</td>\n",
       "      <td>0.132904</td>\n",
       "      <td>0.794607</td>\n",
       "      <td>-0.978691</td>\n",
       "      <td>0.169059</td>\n",
       "      <td>-0.477471</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530</th>\n",
       "      <td>0.005185</td>\n",
       "      <td>0.005185</td>\n",
       "      <td>0.005185</td>\n",
       "      <td>0.994815</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.989630</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.994815</td>\n",
       "      <td>0.994815</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049580</td>\n",
       "      <td>-1.156276</td>\n",
       "      <td>0.642646</td>\n",
       "      <td>-0.736834</td>\n",
       "      <td>-1.140305</td>\n",
       "      <td>-0.034129</td>\n",
       "      <td>0.641446</td>\n",
       "      <td>-0.408973</td>\n",
       "      <td>-0.431840</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.212231</td>\n",
       "      <td>-0.258955</td>\n",
       "      <td>-0.841753</td>\n",
       "      <td>-0.000465</td>\n",
       "      <td>0.569491</td>\n",
       "      <td>-1.351783</td>\n",
       "      <td>0.704874</td>\n",
       "      <td>-1.582993</td>\n",
       "      <td>-0.367727</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.434394</td>\n",
       "      <td>0.565606</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.434394</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.434394</td>\n",
       "      <td>1.434394</td>\n",
       "      <td>0.565606</td>\n",
       "      <td>...</td>\n",
       "      <td>0.859844</td>\n",
       "      <td>1.304509</td>\n",
       "      <td>0.108302</td>\n",
       "      <td>-0.403253</td>\n",
       "      <td>-0.383612</td>\n",
       "      <td>-0.348634</td>\n",
       "      <td>-0.010192</td>\n",
       "      <td>-0.848349</td>\n",
       "      <td>-1.369505</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.305830</td>\n",
       "      <td>-1.902182</td>\n",
       "      <td>-0.419410</td>\n",
       "      <td>0.038995</td>\n",
       "      <td>0.423305</td>\n",
       "      <td>-0.817338</td>\n",
       "      <td>0.689675</td>\n",
       "      <td>-0.514102</td>\n",
       "      <td>-0.307187</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.749417</td>\n",
       "      <td>-1.224900</td>\n",
       "      <td>-0.174436</td>\n",
       "      <td>-1.358967</td>\n",
       "      <td>0.198750</td>\n",
       "      <td>0.726549</td>\n",
       "      <td>2.530628</td>\n",
       "      <td>-1.468798</td>\n",
       "      <td>-0.907894</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>632 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ER       PgR      HER2  TrippleNegative  ChemoGrade  Proliferation  \\\n",
       "612  0.132801  0.132801  0.132801         0.867199    2.734398       1.867199   \n",
       "353  0.000000  0.000000  1.000000         0.000000    3.000000       1.000000   \n",
       "0    0.000000  0.000000  0.000000         1.000000    3.000000       3.000000   \n",
       "97   0.000000  0.000000  0.000000         1.000000    2.000000       1.000000   \n",
       "613  0.636404  0.636404  0.636404         0.363596    1.727193       1.363596   \n",
       "..        ...       ...       ...              ...         ...            ...   \n",
       "530  0.005185  0.005185  0.005185         0.994815    3.000000       2.989630   \n",
       "40   1.000000  1.000000  0.000000         0.000000    2.000000       1.000000   \n",
       "485  0.000000  0.000000  0.434394         0.565606    3.000000       2.434394   \n",
       "25   1.000000  0.000000  1.000000         0.000000    2.000000       1.000000   \n",
       "132  0.000000  0.000000  0.000000         1.000000    3.000000       2.000000   \n",
       "\n",
       "     HistologyType  LNStatus  TumourStage      Gene  ...  PCA_Component_6  \\\n",
       "612            1.0  0.000000     1.265602  0.867199  ...        -0.255615   \n",
       "353            1.0  0.000000     2.000000  0.000000  ...         0.758432   \n",
       "0              1.0  1.000000     2.000000  1.000000  ...         3.077975   \n",
       "97             1.0  1.000000     2.000000  1.000000  ...         1.379061   \n",
       "613            1.0  0.363596     2.272807  0.363596  ...        -0.416776   \n",
       "..             ...       ...          ...       ...  ...              ...   \n",
       "530            1.0  0.000000     3.994815  0.994815  ...         0.049580   \n",
       "40             1.0  1.000000     2.000000  0.000000  ...        -1.212231   \n",
       "485            1.0  0.434394     1.434394  0.565606  ...         0.859844   \n",
       "25             1.0  1.000000     2.000000  1.000000  ...         0.305830   \n",
       "132            1.0  1.000000     4.000000  0.000000  ...        -1.749417   \n",
       "\n",
       "     PCA_Component_7  PCA_Component_8  PCA_Component_9  PCA_Component_10  \\\n",
       "612         2.306287         0.302505        -0.734249         -0.410544   \n",
       "353         0.369647        -0.500542        -0.727722         -0.911664   \n",
       "0           1.328483        -4.604421         0.566355         -1.433317   \n",
       "97          0.275397         1.327597         0.115218          0.247911   \n",
       "613        -0.303488        -0.289124        -0.325761          0.132904   \n",
       "..               ...              ...              ...               ...   \n",
       "530        -1.156276         0.642646        -0.736834         -1.140305   \n",
       "40         -0.258955        -0.841753        -0.000465          0.569491   \n",
       "485         1.304509         0.108302        -0.403253         -0.383612   \n",
       "25         -1.902182        -0.419410         0.038995          0.423305   \n",
       "132        -1.224900        -0.174436        -1.358967          0.198750   \n",
       "\n",
       "     PCA_Component_11  PCA_Component_12  PCA_Component_13  PCA_Component_14  \\\n",
       "612         -0.111068          0.312285         -0.807706         -1.778402   \n",
       "353          0.901332         -0.808491         -0.902036         -1.457106   \n",
       "0           -2.189594          1.413369          0.157920         -3.178941   \n",
       "97           0.802660          0.118586         -0.693596          0.367737   \n",
       "613          0.794607         -0.978691          0.169059         -0.477471   \n",
       "..                ...               ...               ...               ...   \n",
       "530         -0.034129          0.641446         -0.408973         -0.431840   \n",
       "40          -1.351783          0.704874         -1.582993         -0.367727   \n",
       "485         -0.348634         -0.010192         -0.848349         -1.369505   \n",
       "25          -0.817338          0.689675         -0.514102         -0.307187   \n",
       "132          0.726549          2.530628         -1.468798         -0.907894   \n",
       "\n",
       "     pCR (outcome)  \n",
       "612              1  \n",
       "353              0  \n",
       "0                1  \n",
       "97               0  \n",
       "613              1  \n",
       "..             ...  \n",
       "530              1  \n",
       "40               0  \n",
       "485              1  \n",
       "25               0  \n",
       "132              0  \n",
       "\n",
       "[632 rows x 27 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize and run the pipeline\n",
    "pipeline = DataPreprocessingPipeline(file_path='../data/TrainDataset2024.xls', is_training=True, target_column='pCR (outcome)', augmentation_strategy='smote')\n",
    "X_train, X_test, y_train, y_test = pipeline.run_pipeline()\n",
    "\n",
    "# Create DataFrames for train and test sets with their labels\n",
    "train_data = pd.DataFrame(X_train)\n",
    "train_data['pCR (outcome)'] = y_train\n",
    "\n",
    "test_data = pd.DataFrame(X_test)\n",
    "test_data['pCR (outcome)'] = y_test\n",
    "\n",
    "# Concatenate train and test sets\n",
    "combined_data = pd.concat([train_data, test_data], axis=0)\n",
    "print(\"New dataset shape:\", combined_data.shape)\n",
    "display(combined_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split features and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split features and target\n",
    "X = combined_data.drop('pCR (outcome)', axis=1)\n",
    "y = combined_data['pCR (outcome)']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base_models_and_params(random_state=42):\n",
    "    \"\"\"Get base models and their parameter grids.\"\"\"\n",
    "    return {\n",
    "        'Logistic Regression': (\n",
    "            LogisticRegression(random_state=random_state, class_weight='balanced'),\n",
    "            {\n",
    "                'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "                'solver': ['lbfgs', 'liblinear'],\n",
    "                'max_iter': [2000]\n",
    "            }\n",
    "        ),\n",
    "        'MLP': (\n",
    "            MLPClassifier(random_state=random_state, early_stopping=True),\n",
    "            {\n",
    "                'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],\n",
    "                'activation': ['relu', 'tanh'],\n",
    "                'alpha': [0.0001, 0.001, 0.01],\n",
    "                'learning_rate': ['constant', 'adaptive']\n",
    "            }\n",
    "        ),\n",
    "        'SVM': (\n",
    "            SVC(random_state=random_state, class_weight='balanced'),\n",
    "            {\n",
    "                'C': [0.1, 1, 10],\n",
    "                'kernel': ['linear', 'rbf'],\n",
    "                'gamma': ['scale', 'auto', 0.1, 1]\n",
    "            }\n",
    "        ),\n",
    "        'Decision Tree': (\n",
    "            DecisionTreeClassifier(random_state=random_state, class_weight='balanced'),\n",
    "            {\n",
    "                'max_depth': [3, 5, 7, None],\n",
    "                'min_samples_split': [2, 5, 10],\n",
    "                'min_samples_leaf': [1, 2, 4]\n",
    "            }\n",
    "        )\n",
    "    }\n",
    "\n",
    "def evaluate_model(model, X_train, X_test, y_train, y_test, cv):\n",
    "    \"\"\"Evaluate a single model using cross-validation and test set.\"\"\"\n",
    "    cv_scores = []\n",
    "    \n",
    "    # Cross-validation\n",
    "    for train_idx, val_idx in cv.split(X_train, y_train):\n",
    "        model.fit(X_train[train_idx], y_train[train_idx])\n",
    "        y_pred = model.predict(X_train[val_idx])\n",
    "        scores = {\n",
    "            'accuracy': accuracy_score(y_train[val_idx], y_pred),\n",
    "            'precision': precision_score(y_train[val_idx], y_pred),\n",
    "            'recall': recall_score(y_train[val_idx], y_pred),\n",
    "            'f1': f1_score(y_train[val_idx], y_pred)\n",
    "        }\n",
    "        cv_scores.append(scores)\n",
    "    \n",
    "    # Test set evaluation\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    test_scores = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'precision': precision_score(y_test, y_pred),\n",
    "        'recall': recall_score(y_test, y_pred),\n",
    "        'f1': f1_score(y_test, y_pred)\n",
    "    }\n",
    "    \n",
    "    return {'cv_scores': cv_scores, 'test_scores': test_scores}\n",
    "\n",
    "def run_evaluation(X, y, optimize=True):\n",
    "    \"\"\"Run complete evaluation pipeline with optional optimization.\"\"\"\n",
    "    # Prepare data\n",
    "    X, y = np.asarray(X), np.asarray(y)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, stratify=y, random_state=42\n",
    "    )\n",
    "    \n",
    "    cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    results = {}\n",
    "    \n",
    "    # Get models and their parameter grids\n",
    "    models_and_params = get_base_models_and_params()\n",
    "    \n",
    "    # Evaluate each model\n",
    "    for name, (model, param_grid) in models_and_params.items():\n",
    "        if optimize:\n",
    "            print(f\"\\nOptimizing {name}...\")\n",
    "            grid_search = GridSearchCV(\n",
    "                model, param_grid, cv=5, scoring='f1', n_jobs=-1\n",
    "            )\n",
    "            grid_search.fit(X_train, y_train)\n",
    "            model = grid_search.best_estimator_\n",
    "            print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "            print(f\"Best CV score: {grid_search.best_score_:.3f}\")\n",
    "        \n",
    "        results[name] = evaluate_model(\n",
    "            model, X_train, X_test, y_train, y_test, cv\n",
    "        )\n",
    "    \n",
    "    # Create results DataFrames\n",
    "    cv_data = [{\n",
    "        'Model': name,\n",
    "        **{metric: f\"{np.mean([fold[metric] for fold in scores['cv_scores']]):.3f} ± \"\n",
    "                  f\"{np.std([fold[metric] for fold in scores['cv_scores']]):.3f}\"\n",
    "           for metric in ['accuracy', 'precision', 'recall', 'f1']}\n",
    "    } for name, scores in results.items()]\n",
    "    \n",
    "    test_data = [{\n",
    "        'Model': name,\n",
    "        **{metric: f\"{scores['test_scores'][metric]:.3f}\"\n",
    "           for metric in ['accuracy', 'precision', 'recall', 'f1']}\n",
    "    } for name, scores in results.items()]\n",
    "    \n",
    "    # Display results\n",
    "    cv_df = pd.DataFrame(cv_data).set_index('Model')\n",
    "    test_df = pd.DataFrame(test_data).set_index('Model')\n",
    "    \n",
    "    print(\"\\nCross-validation Results:\")\n",
    "    display(cv_df.style.set_properties(**{'text-align': 'center'})\n",
    "           .set_table_styles([{'selector': 'th', 'props': [('text-align', 'center')]}]))\n",
    "    \n",
    "    print(\"\\nTest Set Results:\")\n",
    "    display(test_df.style.set_properties(**{'text-align': 'center'})\n",
    "           .set_table_styles([{'selector': 'th', 'props': [('text-align', 'center')]}]))\n",
    "    \n",
    "    return results, cv_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimizing Logistic Regression...\n",
      "Best parameters: {'C': 1, 'max_iter': 2000, 'solver': 'lbfgs'}\n",
      "Best CV score: 0.735\n",
      "\n",
      "Optimizing MLP...\n",
      "Best parameters: {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (100, 50), 'learning_rate': 'constant'}\n",
      "Best CV score: 0.758\n",
      "\n",
      "Optimizing SVM...\n",
      "Best parameters: {'C': 10, 'gamma': 'auto', 'kernel': 'rbf'}\n",
      "Best CV score: 0.883\n",
      "\n",
      "Optimizing Decision Tree...\n",
      "Best parameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 10}\n",
      "Best CV score: 0.763\n",
      "\n",
      "Cross-validation Results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_3fa86 th {\n",
       "  text-align: center;\n",
       "}\n",
       "#T_3fa86_row0_col0, #T_3fa86_row0_col1, #T_3fa86_row0_col2, #T_3fa86_row0_col3, #T_3fa86_row1_col0, #T_3fa86_row1_col1, #T_3fa86_row1_col2, #T_3fa86_row1_col3, #T_3fa86_row2_col0, #T_3fa86_row2_col1, #T_3fa86_row2_col2, #T_3fa86_row2_col3, #T_3fa86_row3_col0, #T_3fa86_row3_col1, #T_3fa86_row3_col2, #T_3fa86_row3_col3 {\n",
       "  text-align: center;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_3fa86\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_3fa86_level0_col0\" class=\"col_heading level0 col0\" >accuracy</th>\n",
       "      <th id=\"T_3fa86_level0_col1\" class=\"col_heading level0 col1\" >precision</th>\n",
       "      <th id=\"T_3fa86_level0_col2\" class=\"col_heading level0 col2\" >recall</th>\n",
       "      <th id=\"T_3fa86_level0_col3\" class=\"col_heading level0 col3\" >f1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Model</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_3fa86_level0_row0\" class=\"row_heading level0 row0\" >Logistic Regression</th>\n",
       "      <td id=\"T_3fa86_row0_col0\" class=\"data row0 col0\" >0.746 ± 0.046</td>\n",
       "      <td id=\"T_3fa86_row0_col1\" class=\"data row0 col1\" >0.735 ± 0.048</td>\n",
       "      <td id=\"T_3fa86_row0_col2\" class=\"data row0 col2\" >0.775 ± 0.077</td>\n",
       "      <td id=\"T_3fa86_row0_col3\" class=\"data row0 col3\" >0.753 ± 0.049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3fa86_level0_row1\" class=\"row_heading level0 row1\" >MLP</th>\n",
       "      <td id=\"T_3fa86_row1_col0\" class=\"data row1 col0\" >0.759 ± 0.040</td>\n",
       "      <td id=\"T_3fa86_row1_col1\" class=\"data row1 col1\" >0.743 ± 0.064</td>\n",
       "      <td id=\"T_3fa86_row1_col2\" class=\"data row1 col2\" >0.819 ± 0.122</td>\n",
       "      <td id=\"T_3fa86_row1_col3\" class=\"data row1 col3\" >0.769 ± 0.049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3fa86_level0_row2\" class=\"row_heading level0 row2\" >SVM</th>\n",
       "      <td id=\"T_3fa86_row2_col0\" class=\"data row2 col0\" >0.881 ± 0.045</td>\n",
       "      <td id=\"T_3fa86_row2_col1\" class=\"data row2 col1\" >0.863 ± 0.060</td>\n",
       "      <td id=\"T_3fa86_row2_col2\" class=\"data row2 col2\" >0.913 ± 0.030</td>\n",
       "      <td id=\"T_3fa86_row2_col3\" class=\"data row2 col3\" >0.886 ± 0.039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3fa86_level0_row3\" class=\"row_heading level0 row3\" >Decision Tree</th>\n",
       "      <td id=\"T_3fa86_row3_col0\" class=\"data row3 col0\" >0.766 ± 0.064</td>\n",
       "      <td id=\"T_3fa86_row3_col1\" class=\"data row3 col1\" >0.787 ± 0.081</td>\n",
       "      <td id=\"T_3fa86_row3_col2\" class=\"data row3 col2\" >0.739 ± 0.091</td>\n",
       "      <td id=\"T_3fa86_row3_col3\" class=\"data row3 col3\" >0.759 ± 0.070</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x174f2baf0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Set Results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_dbef5 th {\n",
       "  text-align: center;\n",
       "}\n",
       "#T_dbef5_row0_col0, #T_dbef5_row0_col1, #T_dbef5_row0_col2, #T_dbef5_row0_col3, #T_dbef5_row1_col0, #T_dbef5_row1_col1, #T_dbef5_row1_col2, #T_dbef5_row1_col3, #T_dbef5_row2_col0, #T_dbef5_row2_col1, #T_dbef5_row2_col2, #T_dbef5_row2_col3, #T_dbef5_row3_col0, #T_dbef5_row3_col1, #T_dbef5_row3_col2, #T_dbef5_row3_col3 {\n",
       "  text-align: center;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_dbef5\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_dbef5_level0_col0\" class=\"col_heading level0 col0\" >accuracy</th>\n",
       "      <th id=\"T_dbef5_level0_col1\" class=\"col_heading level0 col1\" >precision</th>\n",
       "      <th id=\"T_dbef5_level0_col2\" class=\"col_heading level0 col2\" >recall</th>\n",
       "      <th id=\"T_dbef5_level0_col3\" class=\"col_heading level0 col3\" >f1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Model</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_dbef5_level0_row0\" class=\"row_heading level0 row0\" >Logistic Regression</th>\n",
       "      <td id=\"T_dbef5_row0_col0\" class=\"data row0 col0\" >0.693</td>\n",
       "      <td id=\"T_dbef5_row0_col1\" class=\"data row0 col1\" >0.654</td>\n",
       "      <td id=\"T_dbef5_row0_col2\" class=\"data row0 col2\" >0.810</td>\n",
       "      <td id=\"T_dbef5_row0_col3\" class=\"data row0 col3\" >0.723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_dbef5_level0_row1\" class=\"row_heading level0 row1\" >MLP</th>\n",
       "      <td id=\"T_dbef5_row1_col0\" class=\"data row1 col0\" >0.780</td>\n",
       "      <td id=\"T_dbef5_row1_col1\" class=\"data row1 col1\" >0.746</td>\n",
       "      <td id=\"T_dbef5_row1_col2\" class=\"data row1 col2\" >0.841</td>\n",
       "      <td id=\"T_dbef5_row1_col3\" class=\"data row1 col3\" >0.791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_dbef5_level0_row2\" class=\"row_heading level0 row2\" >SVM</th>\n",
       "      <td id=\"T_dbef5_row2_col0\" class=\"data row2 col0\" >0.858</td>\n",
       "      <td id=\"T_dbef5_row2_col1\" class=\"data row2 col1\" >0.800</td>\n",
       "      <td id=\"T_dbef5_row2_col2\" class=\"data row2 col2\" >0.952</td>\n",
       "      <td id=\"T_dbef5_row2_col3\" class=\"data row2 col3\" >0.870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_dbef5_level0_row3\" class=\"row_heading level0 row3\" >Decision Tree</th>\n",
       "      <td id=\"T_dbef5_row3_col0\" class=\"data row3 col0\" >0.787</td>\n",
       "      <td id=\"T_dbef5_row3_col1\" class=\"data row3 col1\" >0.765</td>\n",
       "      <td id=\"T_dbef5_row3_col2\" class=\"data row3 col2\" >0.825</td>\n",
       "      <td id=\"T_dbef5_row3_col3\" class=\"data row3 col3\" >0.794</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x174f2b790>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run evaluation\n",
    "results, cv_df, test_df = run_evaluation(X, y, optimize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing preprocessing on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Summary:\n",
      "Shape: (3, 120)\n",
      "New dataset shape: (3, 27)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pCR (outcome)</th>\n",
       "      <th>ER</th>\n",
       "      <th>PgR</th>\n",
       "      <th>HER2</th>\n",
       "      <th>TrippleNegative</th>\n",
       "      <th>ChemoGrade</th>\n",
       "      <th>Proliferation</th>\n",
       "      <th>HistologyType</th>\n",
       "      <th>LNStatus</th>\n",
       "      <th>TumourStage</th>\n",
       "      <th>...</th>\n",
       "      <th>PCA_Component_5</th>\n",
       "      <th>PCA_Component_6</th>\n",
       "      <th>PCA_Component_7</th>\n",
       "      <th>PCA_Component_8</th>\n",
       "      <th>PCA_Component_9</th>\n",
       "      <th>PCA_Component_10</th>\n",
       "      <th>PCA_Component_11</th>\n",
       "      <th>PCA_Component_12</th>\n",
       "      <th>PCA_Component_13</th>\n",
       "      <th>PCA_Component_14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.342735e+06</td>\n",
       "      <td>9.228345e+06</td>\n",
       "      <td>-1.293895e+06</td>\n",
       "      <td>5.815474e+06</td>\n",
       "      <td>-6.093675e+06</td>\n",
       "      <td>-2.006505e+06</td>\n",
       "      <td>-1.153095e+06</td>\n",
       "      <td>837867.443460</td>\n",
       "      <td>-997979.710300</td>\n",
       "      <td>2.592124e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.787514e+05</td>\n",
       "      <td>3.786511e+05</td>\n",
       "      <td>-5.293761e+04</td>\n",
       "      <td>2.370056e+05</td>\n",
       "      <td>-2.462757e+05</td>\n",
       "      <td>-7.973069e+04</td>\n",
       "      <td>-4.448908e+04</td>\n",
       "      <td>30189.121622</td>\n",
       "      <td>-39654.471274</td>\n",
       "      <td>1.024651e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.456918e+06</td>\n",
       "      <td>7.304004e+06</td>\n",
       "      <td>-1.025277e+06</td>\n",
       "      <td>4.610810e+06</td>\n",
       "      <td>-4.812573e+06</td>\n",
       "      <td>-1.588671e+06</td>\n",
       "      <td>-8.905133e+05</td>\n",
       "      <td>660230.151524</td>\n",
       "      <td>-791301.738564</td>\n",
       "      <td>2.054329e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  pCR (outcome)   ER  PgR  HER2  TrippleNegative  ChemoGrade  Proliferation  \\\n",
       "0             0  0.0  0.0   0.0              1.0         3.0            3.0   \n",
       "1             0  0.0  0.0   1.0              0.0         2.0            1.0   \n",
       "2             0  0.0  0.0   0.0              1.0         3.0            3.0   \n",
       "\n",
       "   HistologyType  LNStatus  TumourStage  ...  PCA_Component_5  \\\n",
       "0            1.0       0.0          2.0  ...    -4.342735e+06   \n",
       "1            1.0       0.0          3.0  ...    -1.787514e+05   \n",
       "2            1.0       1.0          4.0  ...    -3.456918e+06   \n",
       "\n",
       "   PCA_Component_6  PCA_Component_7  PCA_Component_8  PCA_Component_9  \\\n",
       "0     9.228345e+06    -1.293895e+06     5.815474e+06    -6.093675e+06   \n",
       "1     3.786511e+05    -5.293761e+04     2.370056e+05    -2.462757e+05   \n",
       "2     7.304004e+06    -1.025277e+06     4.610810e+06    -4.812573e+06   \n",
       "\n",
       "   PCA_Component_10  PCA_Component_11  PCA_Component_12  PCA_Component_13  \\\n",
       "0     -2.006505e+06     -1.153095e+06     837867.443460    -997979.710300   \n",
       "1     -7.973069e+04     -4.448908e+04      30189.121622     -39654.471274   \n",
       "2     -1.588671e+06     -8.905133e+05     660230.151524    -791301.738564   \n",
       "\n",
       "   PCA_Component_14  \n",
       "0      2.592124e+06  \n",
       "1      1.024651e+05  \n",
       "2      2.054329e+06  \n",
       "\n",
       "[3 rows x 27 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize and run the pipeline\n",
    "pipeline = DataPreprocessingPipeline(file_path='../data/TestDatasetExample.xls', is_training=False, target_column='pCR (outcome)')\n",
    "X_train, X_test, y_train, y_test = pipeline.run_pipeline()\n",
    "\n",
    "# Create DataFrames for train and test sets with their labels\n",
    "train_data = pd.DataFrame(X_train)\n",
    "train_data['pCR (outcome)'] = y_train\n",
    "\n",
    "test_data = pd.DataFrame(X_test)\n",
    "test_data['pCR (outcome)'] = y_test\n",
    "\n",
    "# Concatenate train and test sets\n",
    "combined_data = pd.concat([train_data, test_data], axis=0)\n",
    "print(\"New dataset shape:\", combined_data.shape)\n",
    "display(combined_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features not common to both datasets:\n",
      "Missing in training dataset: ['ER', 'PgR', 'HER2', 'TrippleNegative', 'ChemoGrade', 'Proliferation', 'HistologyType', 'LNStatus', 'TumourStage', 'Gene', 'RelapseFreeSurvival (outcome)', 'Age', 'PCA_Component_1', 'PCA_Component_2', 'PCA_Component_3', 'PCA_Component_4', 'PCA_Component_5', 'PCA_Component_6', 'PCA_Component_7', 'PCA_Component_8', 'PCA_Component_9', 'PCA_Component_10', 'PCA_Component_11', 'PCA_Component_12', 'PCA_Component_13', 'PCA_Component_14']\n"
     ]
    }
   ],
   "source": [
    "# Check if the preprocessing of both training and test dataset have the same name features\n",
    "train_features = train_data.columns.tolist()\n",
    "test_features = test_data.columns.tolist()\n",
    "\n",
    "# Find the features that are not common to both datasets\n",
    "missing_train_features = [feature for feature in test_features if feature not in train_features]\n",
    "missing_test_features = [feature for feature in train_features if feature not in test_features]\n",
    "\n",
    "# Print the results\n",
    "if missing_train_features or missing_test_features:\n",
    "    print(\"Features not common to both datasets:\")\n",
    "    if missing_train_features:\n",
    "        print(\"Missing in training dataset:\", missing_train_features)\n",
    "    if missing_test_features:\n",
    "        print(\"Missing in test dataset:\", missing_test_features)\n",
    "else:\n",
    "    print(\"Both datasets have the same features.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
